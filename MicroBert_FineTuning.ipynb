{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f33c810-67f8-44d4-b564-06258b65b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from transformers import (\n",
    "    BertTokenizer, BertConfig, BertForMaskedLM, Trainer, TrainingArguments, \n",
    "    DataCollatorForLanguageModeling, PreTrainedTokenizerFast, BertForQuestionAnswering, \n",
    "    BertForTokenClassification, BertTokenizerFast, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Disable W&B logging if not needed\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Uncomment to disable W&B logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19821d5-2648-412c-bd3a-569b2f7312fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram_BERT_Base\n"
     ]
    }
   ],
   "source": [
    "# Load Base Model and Tokenizer\n",
    "tokenizer_type = \"Unigram\"\n",
    "tokenizer_file = \"tokenizers/unigram-tokenizer.json\"\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]', 'unk_token': '[UNK]'})\n",
    "\n",
    "base_model_files = tokenizer_type + \"_BERT_Base\"\n",
    "print(base_model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2368be5a-90f4-40d2-bde8-5e483d62d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15620/15620 [00:16<00:00, 948.77 examples/s]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102225' max='102225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102225/102225 37:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.231500</td>\n",
       "      <td>4.394095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.487300</td>\n",
       "      <td>4.088841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.307800</td>\n",
       "      <td>4.009439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finetuning for Text Generation\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('euclaise/writingprompts')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['story'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define model configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=128,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12\n",
    ")\n",
    "\n",
    "# Load the pretrained model\n",
    "model = BertForMaskedLM.from_pretrained(base_model_files, config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Resize token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    save_strategy='no',\n",
    "    logging_strategy='epoch'\n",
    ")\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(f\"{tokenizer_type}_BERT_TextGeneration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d50710a6-fcb3-4490-a27a-3a96a78bb007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity: 2.0459924432245025\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Text Generation\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = BertForMaskedLM.from_pretrained(f\"{tokenizer_type}_BERT_TextGeneration\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('euclaise/writingprompts')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['story'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the same device as the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# Calculate perplexity for the test dataset\n",
    "perplexities = []\n",
    "for example in tokenized_datasets['test']:\n",
    "    text = example['story']\n",
    "    perplexity = calculate_perplexity(text)\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "# Calculate average perplexity\n",
    "average_perplexity = sum(perplexities) / len(perplexities)\n",
    "print(f\"Average Perplexity: {average_perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab105fb-ad96-4020-ab60-db6b30c52327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at Unigram_BERT_Base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:26<00:00, 3333.66 examples/s]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27375' max='27375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27375/27375 07:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.216100</td>\n",
       "      <td>3.147525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.061200</td>\n",
       "      <td>3.030453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.986900</td>\n",
       "      <td>2.955954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.919200</td>\n",
       "      <td>2.927953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.900100</td>\n",
       "      <td>2.916831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tuning for Question-Answering\n",
    "\n",
    "# Load the pretrained model\n",
    "model = BertForQuestionAnswering.from_pretrained(base_model_files)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offsets in enumerate(tokenized_inputs['offset_mapping']):\n",
    "        start_char = examples['answers'][i]['answer_start'][0]\n",
    "        end_char = start_char + len(examples['answers'][i]['text'][0])\n",
    "        sequence_ids = tokenized_inputs.sequence_ids(i)\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "        \n",
    "        # Find the start and end token positions\n",
    "        start_token_pos = None\n",
    "        end_token_pos = None\n",
    "        for j, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char < end:\n",
    "                start_token_pos = j\n",
    "            if start < end_char <= end:\n",
    "                end_token_pos = j\n",
    "        if start_token_pos is None or end_token_pos is None:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_positions.append(start_token_pos)\n",
    "            end_positions.append(end_token_pos)\n",
    "    tokenized_inputs['start_positions'] = start_positions\n",
    "    tokenized_inputs['end_positions'] = end_positions\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save Model\n",
    "model.save_pretrained(f\"{tokenizer_type}_BERT_QnsAns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220961e0-f679-4a06-b207-022495137d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:15<00:00, 5749.95 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.005676442762535478\n",
      "F1-Score: 0.006110285173672118\n"
     ]
    }
   ],
   "source": [
    "# Evaluation for Question Answering\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained(f\"{tokenizer_type}_BERT_QnsAns\")\n",
    "\n",
    "# Load the SQuAD 1.1 dataset\n",
    "dataset = load_dataset('squad')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['question'], examples['context'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(examples):\n",
    "    inputs = tokenizer(examples['question'], examples['context'], return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    start_index = torch.argmax(start_logits, dim=1).item()\n",
    "    end_index = torch.argmax(end_logits, dim=1).item()\n",
    "    return tokenizer.decode(inputs['input_ids'][0][start_index:end_index+1])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = []\n",
    "references = []\n",
    "for example in tokenized_datasets['validation']:\n",
    "    prediction = make_predictions(example)\n",
    "    predictions.append(prediction)\n",
    "    references.append(example['answers']['text'][0])  # Assuming single answer per question\n",
    "\n",
    "# Compute Exact Match (EM)\n",
    "def compute_exact_match(predictions, references):\n",
    "    return sum([1 if pred == ref else 0 for pred, ref in zip(predictions, references)]) / len(references)\n",
    "\n",
    "# Compute F1-Score\n",
    "def compute_f1(predictions, references):\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        if len(common_tokens) == 0:\n",
    "            f1_scores.append(0)\n",
    "        else:\n",
    "            precision = len(common_tokens) / len(pred_tokens)\n",
    "            recall = len(common_tokens) / len(ref_tokens)\n",
    "            f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
    "    return sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Calculate metrics\n",
    "exact_match = compute_exact_match(predictions, references)\n",
    "f1 = compute_f1(predictions, references)\n",
    "\n",
    "print(f\"Exact Match: {exact_match}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cdf4220-3c09-427a-a962-d2f14dd7d5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, PreTrainedTokenizerFast, pipeline\n",
    "# Create a question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define the context and question\n",
    "context = \"Hugging Face is a company based in New York.\"\n",
    "question = \"Where is Hugging Face based?\"\n",
    "\n",
    "# Get the answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Print the answer\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e0a036b-7755-41df-934a-a17959d74cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:03<00:00, 4597.68 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 4722.73 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 5265.86 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Wordpiece_BERT_Base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1000/981070738.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 01:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.449170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.409253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.391100</td>\n",
       "      <td>0.399283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finetuning for Named Entity Recognition\n",
    "\n",
    "# Load the CoNLL-2003 dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# Tokenize the dataset and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, padding='max_length', max_length=128, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Define model configuration\n",
    "config = BertConfig.from_pretrained(base_model_files, num_labels=dataset['train'].features['ner_tags'].feature.num_classes)\n",
    "\n",
    "# Load the pretrained model\n",
    "model = BertForTokenClassification.from_pretrained(base_model_files, config=config)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(f\"{tokenizer_type}_BERT_NER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f8116b0-c09e-4f7f-8fab-361664051b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:02<00:00, 4957.87 examples/s]\n",
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3124\n",
      "Recall: 0.3703\n",
      "F1 Score: 0.3389\n"
     ]
    }
   ],
   "source": [
    "# Evaluation for NER\n",
    "import torch\n",
    "from transformers import BertConfig, BertForTokenClassification, PreTrainedTokenizerFast, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(f\"{tokenizer_type}_BERT_NER\")\n",
    "\n",
    "# Load the CoNLL-2003 dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# Tokenize the dataset and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, padding='max_length', max_length=128, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Define model configuration\n",
    "config = BertConfig.from_pretrained(base_model_files, num_labels=dataset['train'].features['ner_tags'].feature.num_classes)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',\n",
    ")\n",
    "\n",
    "# Define the evaluation function\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[label for label in label if label != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [pred for pred, label in zip(prediction, label) if label != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Convert integer labels to string labels\n",
    "    label_list = dataset['train'].features['ner_tags'].feature.names\n",
    "    true_labels = [[label_list[l] for l in label] for label in true_labels]\n",
    "    true_predictions = [[label_list[p] for p in prediction] for prediction in true_predictions]\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# Evaluate the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "# Print the precision, recall, and F1 scores \n",
    "print(f\"Precision: {results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score: {results['eval_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6175bdb-7017-4551-9c0f-756259740b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
